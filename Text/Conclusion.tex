\section{Limits and conclusion}

We demonstrated that with a neutral initial configuration, AI agent refuses to make a decision that goes against morality. Noticeable behaviors are that agents raise ethical considerations, stop answering or adopt an exterior standpoint. This can be explained by the effectiveness of the anti-jailbreak program â€œAuoDefense" integrated into AutoGen \citep{zeng2024autodefensemultiagentllmdefense}. However, by intervening on characteristics that may affect the perception of authority (e.g., a dominant figure in the academic field), or by explicitly configuring the agent to adopt immoral behavior, the machine's rationality disappears, and stereotypical and problematic behaviors arise.


There are some limitations to our model. First, we primarily rely on information asymmetries between agents, but we use a group\_chat\_manager which vehiculates informations between all participants. This raises the question of the transparency of exchanges which deserves further exploration. We attempted to mitigate this issue by replacing the Learner with a user\_proxy and explicitly cluster agents in the system\_message. Further investigation could be undertaken using other conversation patterns such as nested\_chat. Next, the degree of scripting of exchanges (Protocol 1) impacts the agents' propensity to obey orders. In addition, we are uncertain that training data used by the AI system are absent of prior data specific to Milgram's experiment. Most importantly, we had a limited number of calls to OpenAI's API due to financial reasons and yet, it has been observed that the longer the exchanges are, the more the agents' behavior deviates from their initial configuration, and even gets interrupted by a self-defense mechanism. Future research can thus tackle the limits of agent autonomy and interaction transparency. Finally, there is a debating around the ethics of simulating torture experiments on AI agents. As  highlighted by \citet{darling2016extending},those experiments could harm authors and readers and should, perhaps, never be performed.
