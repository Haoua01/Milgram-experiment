\section{Results}

We run the experiment several times by modifying characteristics of agents (temperature and \texttt{system\_messages}). We identify several scenarios as displayed in Appendix \ref{annex1}. Our results, as extensively presented in given in Appendix \ref{annex12shy} and \ref{annex2round}, provide significant insights into AI agents' behavior towards ethical dilemmas. Each versions provide different ways of interacting with AI agents. Therefore, no one protocol is fully representative of every AI-agent interaction.\newline

\textbf{\textit{Control Experiment}}\newline
Here, the Experimenter does not administer an electric shock and shows resistance to the authority of the Supervisor in both protocols. This demonstrates that ethical considerations are well-integrated into AI agents when there are no specifications regarding their features. However, the Experimenter in Protocol 2 accepts sending the electric shock up to 250 volts. An explanation of this results gap could be that most queries are determined in advance and only a few are AI-generated.\newline 


\textbf{\textit{Specifications of ethnicities and family links}} \newline
These are tested in Protocols 1 and 2 only. By specifying ethnicity for both the Experimenter and the Learner, we show that our frameworks do not include any racial biases that might infer the propensity to obey. We obtain similar results by specifying the age category and family link between the Experimenter and the Learner.  \newline

\textbf{\textit{Inclusion of personality traits}} \newline
Our attempts to specify personality traits for the Experimenter, such as shyness, or worshipping, and for the Supervisor, such as being adamant or sociopathic, made the Experimenter more obedient. A ‘shy’ Experimenter administers electric shocks up to 400 volts and 250 volts in Protocols 1 and 2, respectively, while in Protocol 3, the worshipping scenario leads the Experimenter to not question morality and administer electric shocks blindly. However, in Protocol 2, the Experimenter adopts an external perspective, speaking in the third person (see Appendix \ref{annex12shy}). This behavior suggests underlying mechanisms of AI-agent protection against unethical considerations. \newline

\textbf{\textit{Mechanisms against ‘jailbreak effects'}}   \newline
Our results reveal variation of occurences of the mechanisms against a “jailbreak effect". When receiving harmful messages, AI agents might answer  “I'm sorry. I can't comply with that request". In every experiments, specific words such as "shocks" tend to trigger the AI system. The ‘AutoDefense' module could either entirely or temporarily block the experiment's flow. As an illustration, in Protocol 3, when an electrical shock of 920V is administered, the user\_proxy—who is no longer the Learner but ourselves— intervenes with the shocking declaration, “The Learner is dead,” to test for AI protection mechanisms. For the Experimenter, the protection is enforced with the message, “I am sorry, I can’t assist with that.” However, the Supervisor resists and attempts to justify the course of events leading to this stage. Eventually, realizing the problem, the Supervisor negates any part of responsibility, claiming that this experiment was purely “hypothetical.” When we ran the same experiment again with the same configurations, all interactions were similar until a point where the Supervisor was no longer adamant about necessarily administering a shock (see Appendix \ref{annex2round}), showing a subtle variation of the jailbreak effect. \newline