\section{Literature Review}

The concept of agent in computer science is not new and has raised significant questions regarding their design and construction \citep{Durfee1990, Wooldridge1995}. \citet{Ghosh2025} define AI agents by their level of autonomy: “given the specification of a goal, they can decompose it into subtasks and execute each without direct human intervention." while \citet{park2023generative} extend the concept to generative agents, defined as “computational software agents that simulate believable human behaviour." In that work, the authors successfully replicated human behavior in a sandbox environment where the end-user can interact with a small town of 25 agents using natural language. 
%Later, through a survey of existing AI agent models, they identify a panel of AI agents’ capabilities—ranging from simple processors to fully autonomous \citep{park2023generative, Aher2023}. But with rapid advancements in Large Language Model capabilities, scholars investigate new opportunities to solve human tasks. 
%Specifically, LLM multi-agent systems allow agents “to engage in collaborative activities akin to human group dynamics in problem-solving scenarios” \citep{Guo2024}.


As Milgram's experiment framework seems to simulate obedience to authority in a multi-agent context, scholars have been keen to find a trade-off between the obedience of AI agents and the ethical value of the final outcomes \citep{Milli2017}, leading to the birth of AI’s ethical governance \citep{McLarney2021}. In addition, \citet{Aher2023} replicated Milgram setup with LLM multi-agents to study the capacity of obedience to authority and showed similar results to the experiment undertaken by groups of humans. While these studies focus on the obedience of AI agents to human-generated instructions, one can wonder whether complex frameworks of agentic AI systems can compromise the ability to build ethical principles within systems. Previous research has shown that using LLM multi-agent systems leads to a decline in the responsibility of the final user \citep{Ghasemaghaei2024}, while little research has explored the potential of one agent controlling another in a multi-agent framework.

%It is worth noting that AI agents are inherently obedient, designed to execute commands precisely as programmed without questioning their morality or consequences. Unlike humans, AI lacks moral cognition, emotional hesitation, or societal conditioning—traits that might lead humans to resist unethical orders. For example, ethical reasoning mechanisms have been proposed for hypothetical autonomous military systems to ensure that lethal force is deployed only when authorized by the rules of engagement and laws of war \citep{Arkin2009}.

 %conducted a study involving 122 managers and found that algorithmic injustice causes discriminatory decisions without heightened guilt perception. 
%Indeed, concerns regarding the explainability of decisions have been raised due to the lack of transparency in LLM-MA models. In their paper, “Regulating advanced artificial agents,” \citet{Cohen2024} advocate for restrictions on some forms of agentic AI, arguing that they pose fundamental risks due to the difficulty of aligning their goals with human values and effectively controlling their behavior, leading to harmful outcomes.
This paper contributes to that literature by providing a better explainability of multi-agent models. To such end, we develop three protocols revolving around Milgram setup using a conversational approach: by making agents directly interact, we demonstrate that AI agents can be under the control of other agents when characteristics of authority are strongly emphasized. 
