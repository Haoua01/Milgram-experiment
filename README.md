**Exploring Machine Obedience: Multi-Agent Frameworks Around the Milgram Experiment**

This project investigates the ethical dynamics of Large Language Models (LLMs) in simulating human behavior, particularly focusing on authoritarian relationships. Inspired by Stanley Milgram's seminal 1963 experiment on obedience, the study examines whether AI agents can resist authoritative instructions and uphold ethical standards within a multi-agent system.
The primary goal of this research is to understand the extent to which AI agents influence each other and the conditions under which they comply with authoritative instructions. By implementing Milgram's framework, we explore the ethical implications of AI agents in social science applications.

*Methodology*
The study is based on the Milgram Experiment, which examines destructive obedience under authoritative pressure. 
We have developed three protocols using the AutGen Python module and Open AI API keys to simulate interactions between multiple AI agents.

*Acknowledgments*
Inspired by Stanley Milgram's experiment on obedience.
Utilizes the AutoGen framework and OpenAI's API for experimentation.
